{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fdf8766",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from keras import layers, preprocessing, callbacks, losses, utils, models, regularizers\n",
    "import sys\n",
    "import PyQt5\n",
    "from PyQt5.QtWidgets import QApplication, QLabel, QWidget, QVBoxLayout, QHBoxLayout, QGridLayout,QFrame\n",
    "from PyQt5.QtGui import QPixmap, QPainter, QPen, QColor\n",
    "from PyQt5.QtCore import Qt\n",
    "import time\n",
    "import glob\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79400eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU d√©tect√© : [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "# D√©tection GPU\n",
    "devices = tf.config.list_physical_devices()\n",
    "gpu_devices = [d for d in devices if d.device_type == \"GPU\"]\n",
    "if gpu_devices:\n",
    "    print(\"GPU d√©tect√© :\", gpu_devices)\n",
    "else:\n",
    "    print(\"Aucun GPU Apple d√©tect√©, utilisation du CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561e6f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 1) Parsing des fichiers\n",
    "# =========================\n",
    "\n",
    "def load_sequence_txt(path):\n",
    "    feats = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            # D√©tection automatique du s√©parateur\n",
    "            if \";\" in line:\n",
    "                parts = [p for p in line.split(\";\") if p.strip() != \"\"]\n",
    "            else:\n",
    "                parts = [p for p in line.split() if p.strip() != \"\"]\n",
    "            try:\n",
    "                vec = list(map(float, parts))\n",
    "            except ValueError:\n",
    "                print(f\"Ligne ignor√©e dans {path}: {line[:50]}...\")\n",
    "                continue\n",
    "            feats.append(vec)\n",
    "    X = np.array(feats, dtype=np.float32)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4970997e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 2) Pr√©traitements\n",
    "# =========================\n",
    "\n",
    "def normalize_framewise(X):\n",
    "    Xn = X.copy()\n",
    "    mu = Xn.mean(axis=0, keepdims=True)      # (1, D)\n",
    "    sigma = Xn.std(axis=0, keepdims=True) + 1e-8\n",
    "    Xn = (Xn - mu) / sigma\n",
    "    return Xn\n",
    "\n",
    "def load_dataset_from_folder(seq_folder):\n",
    "    sequences = sorted(glob.glob(os.path.join(seq_folder, \"*.txt\")))\n",
    "    X_list, y_list = [], []\n",
    "    label_to_id = {}\n",
    "    next_id = 0\n",
    "\n",
    "    for path in sequences:\n",
    "        label = os.path.splitext(os.path.basename(path))[0]\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                parts = [p for p in line.split() if p.strip() != \"\"]\n",
    "                try:\n",
    "                    vec = list(map(float, parts))\n",
    "                except ValueError:\n",
    "                    continue\n",
    "                X_list.append(vec)\n",
    "                if label not in label_to_id:\n",
    "                    label_to_id[label] = next_id\n",
    "                    next_id += 1\n",
    "                y_list.append(label_to_id[label])\n",
    "\n",
    "    X = np.array(X_list, dtype=np.float32)\n",
    "    y = np.array(y_list, dtype=np.int64)\n",
    "\n",
    "    id_to_label = {v: k for k, v in label_to_id.items()}\n",
    "    class_names = [id_to_label[i] for i in range(len(id_to_label))]\n",
    "\n",
    "    print(f\"Dataset charg√© : {len(X)} exemples, {X.shape[1]} coordonn√©es, classes = {class_names}\")\n",
    "    return X, y, class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55b6e694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 4) Mod√®le (BatchNorm + Dropout + L2)\n",
    "# =========================\n",
    "\n",
    "def make_model(D, C):\n",
    "    reg = regularizers.l2(1e-4)\n",
    "    model = keras.Sequential([\n",
    "        layers.Input(shape=(D,)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(256, activation='relu', kernel_regularizer=reg),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(128, activation='relu', kernel_regularizer=reg),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(C, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29aea3e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset charg√© : 138728 exemples, 63 coordonn√©es, classes = ['call', 'dislike', 'fist', 'four', 'like']\n",
      "Dataset: (138728, 63) (138728,) ['call', 'dislike', 'fist', 'four', 'like']\n",
      "‚úÖ Normalisation globale appliqu√©e (moyenne et √©cart-type sauvegard√©s).\n",
      "‚öôÔ∏è  Aucun mod√®le trouv√©, entra√Ænement en cours...\n",
      "Epoch 1/20\n",
      "3902/3902 [==============================] - 56s 14ms/step - loss: 0.3190 - accuracy: 0.8872 - val_loss: 0.2233 - val_accuracy: 0.9173\n",
      "Epoch 2/20\n",
      "3902/3902 [==============================] - 51s 13ms/step - loss: 0.2558 - accuracy: 0.9063 - val_loss: 0.2163 - val_accuracy: 0.9191\n",
      "Epoch 3/20\n",
      "3902/3902 [==============================] - 52s 13ms/step - loss: 0.2439 - accuracy: 0.9094 - val_loss: 0.2132 - val_accuracy: 0.9198\n",
      "Epoch 4/20\n",
      "3902/3902 [==============================] - 52s 13ms/step - loss: 0.2362 - accuracy: 0.9122 - val_loss: 0.2067 - val_accuracy: 0.9211\n",
      "Epoch 5/20\n",
      "3902/3902 [==============================] - 52s 13ms/step - loss: 0.2309 - accuracy: 0.9126 - val_loss: 0.2070 - val_accuracy: 0.9203\n",
      "Epoch 6/20\n",
      "3902/3902 [==============================] - 51s 13ms/step - loss: 0.2267 - accuracy: 0.9135 - val_loss: 0.2066 - val_accuracy: 0.9213\n",
      "Epoch 7/20\n",
      "3902/3902 [==============================] - 51s 13ms/step - loss: 0.2246 - accuracy: 0.9136 - val_loss: 0.2016 - val_accuracy: 0.9229\n",
      "Epoch 8/20\n",
      "3902/3902 [==============================] - 51s 13ms/step - loss: 0.2217 - accuracy: 0.9146 - val_loss: 0.2018 - val_accuracy: 0.9210\n",
      "Epoch 9/20\n",
      "3902/3902 [==============================] - 51s 13ms/step - loss: 0.2172 - accuracy: 0.9151 - val_loss: 0.1992 - val_accuracy: 0.9214\n",
      "Epoch 10/20\n",
      "3902/3902 [==============================] - 49s 12ms/step - loss: 0.2176 - accuracy: 0.9155 - val_loss: 0.1969 - val_accuracy: 0.9222\n",
      "Epoch 11/20\n",
      "3902/3902 [==============================] - 51s 13ms/step - loss: 0.2176 - accuracy: 0.9151 - val_loss: 0.1995 - val_accuracy: 0.9209\n",
      "Epoch 12/20\n",
      "3902/3902 [==============================] - 52s 13ms/step - loss: 0.2156 - accuracy: 0.9157 - val_loss: 0.1984 - val_accuracy: 0.9227\n",
      "Epoch 13/20\n",
      "3902/3902 [==============================] - 50s 13ms/step - loss: 0.2144 - accuracy: 0.9158 - val_loss: 0.1960 - val_accuracy: 0.9216\n",
      "Epoch 14/20\n",
      "3902/3902 [==============================] - 52s 13ms/step - loss: 0.2120 - accuracy: 0.9170 - val_loss: 0.1934 - val_accuracy: 0.9218\n",
      "Epoch 15/20\n",
      "3902/3902 [==============================] - 53s 14ms/step - loss: 0.2128 - accuracy: 0.9159 - val_loss: 0.1931 - val_accuracy: 0.9217\n",
      "Epoch 16/20\n",
      "3902/3902 [==============================] - 53s 14ms/step - loss: 0.2107 - accuracy: 0.9164 - val_loss: 0.1953 - val_accuracy: 0.9215\n",
      "Epoch 17/20\n",
      "3902/3902 [==============================] - 52s 13ms/step - loss: 0.2106 - accuracy: 0.9165 - val_loss: 0.1929 - val_accuracy: 0.9219\n",
      "Epoch 18/20\n",
      "3902/3902 [==============================] - 52s 13ms/step - loss: 0.2097 - accuracy: 0.9171 - val_loss: 0.1980 - val_accuracy: 0.9221\n",
      "Epoch 19/20\n",
      "3902/3902 [==============================] - 51s 13ms/step - loss: 0.2090 - accuracy: 0.9163 - val_loss: 0.1906 - val_accuracy: 0.9230\n",
      "Epoch 20/20\n",
      "3902/3902 [==============================] - 51s 13ms/step - loss: 0.2079 - accuracy: 0.9174 - val_loss: 0.1900 - val_accuracy: 0.9229\n",
      "‚è∞ Temps d'entra√Ænement : 1033.55 secondes (17.23 min)\n",
      "üíæ Mod√®le sauvegard√© sous : modele_gestes_20x32_90%.keras\n",
      "üíæ Noms de classes sauvegard√©s dans class_names.npy\n"
     ]
    }
   ],
   "source": [
    "# ================\n",
    "# 5) Entra√Ænement\n",
    "# ================\n",
    "\n",
    "# Dossier o√π se trouvent les fichiers *.txt\n",
    "seq_folder = \"/Users/valentindaveau/Documents/UE_Vision/Coords\" \n",
    "\n",
    "# Construire le dataset\n",
    "X, y, class_names = load_dataset_from_folder(seq_folder)\n",
    "print(\"Dataset:\", X.shape, y.shape, class_names)\n",
    "\n",
    "train_val=0.9\n",
    "\n",
    "# Split simple (train/val)\n",
    "idx = np.arange(len(X))\n",
    "np.random.shuffle(idx)\n",
    "split = int(train_val * len(X))\n",
    "tr, va = idx[:split], idx[split:]\n",
    "Xtr, ytr = X[tr], y[tr]\n",
    "Xva, yva = X[va], y[va]\n",
    "\n",
    "# Normalisation globale bas√©e sur le train\n",
    "mu = Xtr.mean(axis=0, keepdims=True)\n",
    "sigma = Xtr.std(axis=0, keepdims=True) + 1e-8\n",
    "Xtr = (Xtr - mu) / sigma\n",
    "Xva = (Xva - mu) / sigma\n",
    "np.save(\"norm_mu.npy\", mu)\n",
    "np.save(\"norm_sigma.npy\", sigma)\n",
    "print(\"Normalisation globale appliqu√©e (moyenne et √©cart-type sauvegard√©s).\")\n",
    "\n",
    "D = X.shape[1]\n",
    "C = len(class_names)\n",
    "\n",
    "model_path = \"modele_gestes_20x32_90%.keras\"\n",
    "\n",
    "if os.path.exists(model_path):\n",
    "    print(\"Mod√®le trouv√©, chargement du mod√®le sauvegard√©...\")\n",
    "    model = keras.models.load_model(model_path)\n",
    "    class_names = np.load(\"class_names.npy\", allow_pickle=True).tolist()\n",
    "    print(\"Noms de classes charg√©s :\", class_names)\n",
    "else:\n",
    "    print(\"Aucun mod√®le trouv√©, entra√Ænement en cours...\")\n",
    "    model = make_model(D,C)\n",
    "    start_time = time.time()\n",
    "    history = model.fit(Xtr, ytr, validation_data=(Xva, yva), epochs=20, batch_size=32)\n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "    print(f\"Temps d'entra√Ænement : {training_time:.2f} secondes ({training_time/60:.2f} min)\")\n",
    "    model.save(model_path)\n",
    "    print(f\"Mod√®le sauvegard√© sous : {model_path}\")\n",
    "    np.save(\"class_names.npy\", class_names)\n",
    "    print(\"Noms de classes sauvegard√©s dans class_names.npy\")\n",
    "\n",
    "\n",
    "\n",
    "def normalize_landmarks(X20: np.ndarray) -> np.ndarray:\n",
    "    wrist = X20[0]\n",
    "    Xc = X20 - wrist\n",
    "    # distance de ref: WRIST (0) -> MIDDLE MCP (index MediaPipe 9, devenu X20[8] apr√®s mapping)\n",
    "    scale = np.linalg.norm(Xc[8]) + 1e-8\n",
    "    return Xc / scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd473857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Classes connues par le mod√®le : ['call', 'dislike', 'fist', 'four', 'like']\n",
      "Nombre total de classes : 5\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 6) Chargement et informations du mod√®le\n",
    "# =========================\n",
    "\n",
    "model = keras.models.load_model(\"modele_gestes_40x32.keras\")\n",
    "model.summary()\n",
    "class_names = np.load(\"class_names.npy\", allow_pickle=True).tolist()\n",
    "mu = np.load(\"norm_mu.npy\")\n",
    "sigma = np.load(\"norm_sigma.npy\")\n",
    "\n",
    "class_names = np.load(\"class_names.npy\", allow_pickle=True).tolist()\n",
    "print(\"Classes connues par le mod√®le :\", class_names)\n",
    "print(\"Nombre total de classes :\", len(class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b40385b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1762792376.253183 10452505 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 90.5), renderer: Apple M2\n",
      "W0000 00:00:1762792376.389879 10475008 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1762792376.406482 10475012 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé• Cam√©ra ouverte √† l'index 0\n",
      "üé• Cam√©ra pr√™te. Appuie sur ESC pour quitter.\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 7) Test √† la cam√©ra\n",
    "# =========================\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "hands = mp_hands.Hands(\n",
    "    static_image_mode=False,\n",
    "    max_num_hands=1,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "\n",
    "# Ouverture cam√©ra\n",
    "cap = None\n",
    "for i in [0, 1]:\n",
    "    cap = cv2.VideoCapture(i, cv2.CAP_AVFOUNDATION)\n",
    "    if cap.isOpened():\n",
    "        print(f\"Cam√©ra ouverte √† l'index {i}\")\n",
    "        break\n",
    "if cap is None or not cap.isOpened():\n",
    "    raise RuntimeError(\"Aucune cam√©ra utilisable d√©tect√©e.\")\n",
    "\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "cap.set(cv2.CAP_PROP_FPS, 30)\n",
    "time.sleep(1.0)\n",
    "\n",
    "cv2.namedWindow(\"Reconnaissance de gestes\", cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow(\"Reconnaissance de gestes\", 900, 700)\n",
    "\n",
    "last_pred_time = 0\n",
    "pred_interval = 0.3\n",
    "last_preds = None\n",
    "last_top3_idx = None\n",
    "\n",
    "print(\"Cam√©ra pr√™te. Appuie sur ESC pour quitter.\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"cap.read() a √©chou√©.\")\n",
    "        break\n",
    "\n",
    "    image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(image_rgb)\n",
    "\n",
    "    out = frame.copy()\n",
    "    now = time.time()\n",
    "\n",
    "    # --- D√©tection et pr√©diction ---\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(out, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "            coords = np.array([[lm.x, lm.y, lm.z] for lm in hand_landmarks.landmark], dtype=np.float32).flatten()\n",
    "            coords = (coords - mu.flatten()) / sigma.flatten()\n",
    "\n",
    "            # Nouvelle pr√©diction une fois par seconde\n",
    "            if now - last_pred_time > pred_interval:\n",
    "                preds = model.predict(np.expand_dims(coords, axis=0), verbose=0)[0]\n",
    "                top3_idx = preds.argsort()[-3:][::-1]\n",
    "                last_preds = preds\n",
    "                last_top3_idx = top3_idx\n",
    "                last_pred_time = now\n",
    "\n",
    "    # --- Affichage du dernier r√©sultat connu (√† chaque frame) ---\n",
    "    if last_preds is not None and last_top3_idx is not None:\n",
    "        for i, idx in enumerate(last_top3_idx):\n",
    "            text = f\"{class_names[idx]} : {last_preds[idx]*100:.1f}%\"\n",
    "            y_pos = 40 + i * 35\n",
    "            color = (0, 255, 0) if i == 0 else (255, 255, 255)\n",
    "            cv2.putText(out, text, (20, y_pos),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2, cv2.LINE_AA)\n",
    "\n",
    "    # --- Affichage cam√©ra ---\n",
    "    cv2.imshow(\"Reconnaissance de gestes\", out)\n",
    "    if cv2.waitKey(5) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfmetal310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
