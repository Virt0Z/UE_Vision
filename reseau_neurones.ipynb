{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fdf8766",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from keras import layers, preprocessing, callbacks, losses, utils, models, regularizers\n",
    "import sys\n",
    "import PyQt5\n",
    "from PyQt5.QtWidgets import QApplication, QLabel, QWidget, QVBoxLayout, QHBoxLayout, QGridLayout,QFrame\n",
    "from PyQt5.QtGui import QPixmap, QPainter, QPen, QColor\n",
    "from PyQt5.QtCore import Qt\n",
    "import time\n",
    "import glob\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79400eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU détecté : [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "# Détection GPU\n",
    "devices = tf.config.list_physical_devices()\n",
    "gpu_devices = [d for d in devices if d.device_type == \"GPU\"]\n",
    "if gpu_devices:\n",
    "    print(\"GPU détecté :\", gpu_devices)\n",
    "else:\n",
    "    print(\"Aucun GPU Apple détecté, utilisation du CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561e6f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 1) Parsing des fichiers\n",
    "# =========================\n",
    "\n",
    "def load_sequence_txt(path):\n",
    "    feats = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            # Détection automatique du séparateur\n",
    "            if \";\" in line:\n",
    "                parts = [p for p in line.split(\";\") if p.strip() != \"\"]\n",
    "            else:\n",
    "                parts = [p for p in line.split() if p.strip() != \"\"]\n",
    "            try:\n",
    "                vec = list(map(float, parts))\n",
    "            except ValueError:\n",
    "                print(f\"Ligne ignorée dans {path}: {line[:50]}...\")\n",
    "                continue\n",
    "            feats.append(vec)\n",
    "    X = np.array(feats, dtype=np.float32)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4970997e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 2) Prétraitements\n",
    "# =========================\n",
    "\n",
    "def normalize_framewise(X):\n",
    "    Xn = X.copy()\n",
    "    mu = Xn.mean(axis=0, keepdims=True)      # (1, D)\n",
    "    sigma = Xn.std(axis=0, keepdims=True) + 1e-8\n",
    "    Xn = (Xn - mu) / sigma\n",
    "    return Xn\n",
    "\n",
    "def load_dataset_from_folder(seq_folder):\n",
    "    sequences = sorted(glob.glob(os.path.join(seq_folder, \"*.txt\")))\n",
    "    X_list, y_list = [], []\n",
    "    label_to_id = {}\n",
    "    next_id = 0\n",
    "\n",
    "    for path in sequences:\n",
    "        label = os.path.splitext(os.path.basename(path))[0]\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                parts = [p for p in line.split() if p.strip() != \"\"]\n",
    "                try:\n",
    "                    vec = list(map(float, parts))\n",
    "                except ValueError:\n",
    "                    continue\n",
    "                X_list.append(vec)\n",
    "                if label not in label_to_id:\n",
    "                    label_to_id[label] = next_id\n",
    "                    next_id += 1\n",
    "                y_list.append(label_to_id[label])\n",
    "\n",
    "    X = np.array(X_list, dtype=np.float32)\n",
    "    y = np.array(y_list, dtype=np.int64)\n",
    "\n",
    "    id_to_label = {v: k for k, v in label_to_id.items()}\n",
    "    class_names = [id_to_label[i] for i in range(len(id_to_label))]\n",
    "\n",
    "    print(f\"Dataset chargé : {len(X)} exemples, {X.shape[1]} coordonnées, classes = {class_names}\")\n",
    "    return X, y, class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55b6e694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 4) Modèle (BatchNorm + Dropout + L2)\n",
    "# =========================\n",
    "\n",
    "def make_model(D, C):\n",
    "    reg = regularizers.l2(1e-4)\n",
    "    model = keras.Sequential([\n",
    "        layers.Input(shape=(D,)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(256, activation='relu', kernel_regularizer=reg),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(128, activation='relu', kernel_regularizer=reg),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(C, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29aea3e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset chargé : 452428 exemples, 63 coordonnées, classes = ['call', 'dislike', 'fist', 'four', 'like', 'mute', 'ok', 'one', 'peace', 'peace_inverted', 'rock', 'stop', 'stop_inverted', 'three', 'three3', 'thumb_index2']\n",
      "Dataset: (452428, 63) (452428,) ['call', 'dislike', 'fist', 'four', 'like', 'mute', 'ok', 'one', 'peace', 'peace_inverted', 'rock', 'stop', 'stop_inverted', 'three', 'three3', 'thumb_index2']\n",
      "Normalisation globale appliquée (moyenne et écart-type sauvegardés).\n",
      "Aucun modèle trouvé, entraînement en cours...\n",
      "Metal device set to: Apple M2\n",
      "\n",
      "systemMemory: 8.00 GB\n",
      "maxCacheSize: 2.67 GB\n",
      "\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-10 20:11:27.348216: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11311/11311 [==============================] - 188s 17ms/step - loss: 0.5096 - accuracy: 0.8539 - val_loss: 0.3607 - val_accuracy: 0.9024\n",
      "Epoch 2/20\n",
      "11311/11311 [==============================] - 176s 16ms/step - loss: 0.4075 - accuracy: 0.8873 - val_loss: 0.3426 - val_accuracy: 0.9066\n",
      "Epoch 3/20\n",
      "11311/11311 [==============================] - 175s 15ms/step - loss: 0.3899 - accuracy: 0.8920 - val_loss: 0.3327 - val_accuracy: 0.9078\n",
      "Epoch 4/20\n",
      "11311/11311 [==============================] - 183s 16ms/step - loss: 0.3765 - accuracy: 0.8951 - val_loss: 0.3245 - val_accuracy: 0.9093\n",
      "Epoch 5/20\n",
      "11311/11311 [==============================] - 167s 15ms/step - loss: 0.3698 - accuracy: 0.8962 - val_loss: 0.3218 - val_accuracy: 0.9102\n",
      "Epoch 6/20\n",
      "11311/11311 [==============================] - 165s 15ms/step - loss: 0.3655 - accuracy: 0.8971 - val_loss: 0.3199 - val_accuracy: 0.9106\n",
      "Epoch 7/20\n",
      "11311/11311 [==============================] - 165s 15ms/step - loss: 0.3609 - accuracy: 0.8980 - val_loss: 0.3178 - val_accuracy: 0.9107\n",
      "Epoch 8/20\n",
      "11311/11311 [==============================] - 172s 15ms/step - loss: 0.3582 - accuracy: 0.8985 - val_loss: 0.3171 - val_accuracy: 0.9100\n",
      "Epoch 9/20\n",
      "11311/11311 [==============================] - 167s 15ms/step - loss: 0.3573 - accuracy: 0.8985 - val_loss: 0.3170 - val_accuracy: 0.9101\n",
      "Epoch 10/20\n",
      "11311/11311 [==============================] - 166s 15ms/step - loss: 0.3545 - accuracy: 0.8993 - val_loss: 0.3115 - val_accuracy: 0.9118\n",
      "Epoch 11/20\n",
      "11311/11311 [==============================] - 170s 15ms/step - loss: 0.3529 - accuracy: 0.8995 - val_loss: 0.3125 - val_accuracy: 0.9111\n",
      "Epoch 12/20\n",
      "11311/11311 [==============================] - 166s 15ms/step - loss: 0.3524 - accuracy: 0.8999 - val_loss: 0.3128 - val_accuracy: 0.9104\n",
      "Epoch 13/20\n",
      "11311/11311 [==============================] - 170s 15ms/step - loss: 0.3514 - accuracy: 0.8998 - val_loss: 0.3091 - val_accuracy: 0.9113\n",
      "Epoch 14/20\n",
      "11311/11311 [==============================] - 175s 15ms/step - loss: 0.3509 - accuracy: 0.9005 - val_loss: 0.3103 - val_accuracy: 0.9107\n",
      "Epoch 15/20\n",
      "11311/11311 [==============================] - 156s 14ms/step - loss: 0.3499 - accuracy: 0.8999 - val_loss: 0.3099 - val_accuracy: 0.9117\n",
      "Epoch 16/20\n",
      "11311/11311 [==============================] - 160s 14ms/step - loss: 0.3492 - accuracy: 0.9007 - val_loss: 0.3095 - val_accuracy: 0.9117\n",
      "Epoch 17/20\n",
      "11311/11311 [==============================] - 154s 14ms/step - loss: 0.3489 - accuracy: 0.9005 - val_loss: 0.3110 - val_accuracy: 0.9110\n",
      "Epoch 18/20\n",
      "11311/11311 [==============================] - 165s 15ms/step - loss: 0.3490 - accuracy: 0.9004 - val_loss: 0.3096 - val_accuracy: 0.9115\n",
      "Epoch 19/20\n",
      "11311/11311 [==============================] - 161s 14ms/step - loss: 0.3479 - accuracy: 0.9007 - val_loss: 0.3093 - val_accuracy: 0.9120\n",
      "Epoch 20/20\n",
      "11311/11311 [==============================] - 174s 15ms/step - loss: 0.3482 - accuracy: 0.9007 - val_loss: 0.3081 - val_accuracy: 0.9119\n",
      "Temps d'entraînement : 3376.71 secondes (56.28 min)\n",
      "Modèle sauvegardé sous : modele_gestes_20x32_80%_16g.keras\n",
      "Noms de classes sauvegardés dans class_names.npy\n"
     ]
    }
   ],
   "source": [
    "# ================\n",
    "# 5) Entraînement\n",
    "# ================\n",
    "\n",
    "# Dossier où se trouvent les fichiers *.txt\n",
    "seq_folder = \"/Users/valentindaveau/Documents/UE_Vision/Coords\" \n",
    "\n",
    "# Construire le dataset\n",
    "X, y, class_names = load_dataset_from_folder(seq_folder)\n",
    "print(\"Dataset:\", X.shape, y.shape, class_names)\n",
    "\n",
    "train_val=0.8\n",
    "\n",
    "# Split simple (train/val)\n",
    "idx = np.arange(len(X))\n",
    "np.random.shuffle(idx)\n",
    "split = int(train_val * len(X))\n",
    "tr, va = idx[:split], idx[split:]\n",
    "Xtr, ytr = X[tr], y[tr]\n",
    "Xva, yva = X[va], y[va]\n",
    "\n",
    "# Normalisation globale basée sur le train\n",
    "mu = Xtr.mean(axis=0, keepdims=True)\n",
    "sigma = Xtr.std(axis=0, keepdims=True) + 1e-8\n",
    "Xtr = (Xtr - mu) / sigma\n",
    "Xva = (Xva - mu) / sigma\n",
    "np.save(\"norm_mu.npy\", mu)\n",
    "np.save(\"norm_sigma.npy\", sigma)\n",
    "print(\"Normalisation globale appliquée (moyenne et écart-type sauvegardés).\")\n",
    "\n",
    "D = X.shape[1]\n",
    "C = len(class_names)\n",
    "\n",
    "model_path = \"modele_gestes_20x32_80%_16g.keras\"\n",
    "\n",
    "if os.path.exists(model_path):\n",
    "    print(\"Modèle trouvé, chargement du modèle sauvegardé...\")\n",
    "    model = keras.models.load_model(model_path)\n",
    "    class_names = np.load(\"class_names.npy\", allow_pickle=True).tolist()\n",
    "    print(\"Noms de classes chargés :\", class_names)\n",
    "else:\n",
    "    print(\"Aucun modèle trouvé, entraînement en cours...\")\n",
    "    model = make_model(D,C)\n",
    "    start_time = time.time()\n",
    "    history = model.fit(Xtr, ytr, validation_data=(Xva, yva), epochs=20, batch_size=32)\n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "    print(f\"Temps d'entraînement : {training_time:.2f} secondes ({training_time/60:.2f} min)\")\n",
    "    model.save(model_path)\n",
    "    print(f\"Modèle sauvegardé sous : {model_path}\")\n",
    "    np.save(\"class_names.npy\", class_names)\n",
    "    print(\"Noms de classes sauvegardés dans class_names.npy\")\n",
    "\n",
    "\n",
    "\n",
    "def normalize_landmarks(X20: np.ndarray) -> np.ndarray:\n",
    "    wrist = X20[0]\n",
    "    Xc = X20 - wrist\n",
    "    # distance de ref: WRIST (0) -> MIDDLE MCP (index MediaPipe 9, devenu X20[8] après mapping)\n",
    "    scale = np.linalg.norm(Xc[8]) + 1e-8\n",
    "    return Xc / scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd473857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M2\n",
      "\n",
      "systemMemory: 8.00 GB\n",
      "maxCacheSize: 2.67 GB\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 63)               252       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               16384     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 256)              1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 16)                2064      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 52,620\n",
      "Trainable params: 51,982\n",
      "Non-trainable params: 638\n",
      "_________________________________________________________________\n",
      "Classes connues par le modèle : ['call', 'dislike', 'fist', 'four', 'like', 'mute', 'ok', 'one', 'peace', 'peace_inverted', 'rock', 'stop', 'stop_inverted', 'three', 'three3', 'thumb_index2']\n",
      "Nombre total de classes : 16\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 6) Chargement et informations du modèle\n",
    "# =========================\n",
    "\n",
    "model = keras.models.load_model(\"modele_gestes_20x32_80%_16g.keras\")\n",
    "model.summary()\n",
    "class_names = np.load(\"class_names.npy\", allow_pickle=True).tolist()\n",
    "mu = np.load(\"norm_mu.npy\")\n",
    "sigma = np.load(\"norm_sigma.npy\")\n",
    "\n",
    "class_names = np.load(\"class_names.npy\", allow_pickle=True).tolist()\n",
    "print(\"Classes connues par le modèle :\", class_names)\n",
    "print(\"Nombre total de classes :\", len(class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b40385b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1762806266.913417 10660802 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 90.5), renderer: Apple M2\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1762806266.990963 10661283 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1762806267.006066 10661283 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caméra ouverte à l'index 0\n",
      "Caméra prête. Appuie sur ESC pour quitter.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1762806270.094705 10661286 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n",
      "2025-11-10 21:24:30.240357: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 7) Test à la caméra\n",
    "# =========================\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "hands = mp_hands.Hands(\n",
    "    static_image_mode=False,\n",
    "    max_num_hands=1,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "\n",
    "# Ouverture caméra\n",
    "cap = None\n",
    "for i in [0, 1]:\n",
    "    cap = cv2.VideoCapture(i, cv2.CAP_AVFOUNDATION)\n",
    "    if cap.isOpened():\n",
    "        print(f\"Caméra ouverte à l'index {i}\")\n",
    "        break\n",
    "if cap is None or not cap.isOpened():\n",
    "    raise RuntimeError(\"Aucune caméra utilisable détectée.\")\n",
    "\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "cap.set(cv2.CAP_PROP_FPS, 30)\n",
    "time.sleep(1.0)\n",
    "\n",
    "cv2.namedWindow(\"Reconnaissance de gestes\", cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow(\"Reconnaissance de gestes\", 900, 700)\n",
    "\n",
    "last_pred_time = 0\n",
    "pred_interval = 0.3\n",
    "last_preds = None\n",
    "last_top3_idx = None\n",
    "\n",
    "print(\"Caméra prête. Appuie sur ESC pour quitter.\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"cap.read() a échoué.\")\n",
    "        break\n",
    "\n",
    "    image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(image_rgb)\n",
    "\n",
    "    out = frame.copy()\n",
    "    now = time.time()\n",
    "\n",
    "    # --- Détection et prédiction ---\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(out, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "            coords = np.array([[lm.x, lm.y, lm.z] for lm in hand_landmarks.landmark], dtype=np.float32).flatten()\n",
    "            coords = (coords - mu.flatten()) / sigma.flatten()\n",
    "\n",
    "            # Nouvelle prédiction une fois par seconde\n",
    "            if now - last_pred_time > pred_interval:\n",
    "                preds = model.predict(np.expand_dims(coords, axis=0), verbose=0)[0]\n",
    "                top3_idx = preds.argsort()[-3:][::-1]\n",
    "                last_preds = preds\n",
    "                last_top3_idx = top3_idx\n",
    "                last_pred_time = now\n",
    "\n",
    "    # --- Affichage du dernier résultat connu (à chaque frame) ---\n",
    "    if last_preds is not None and last_top3_idx is not None:\n",
    "        for i, idx in enumerate(last_top3_idx):\n",
    "            text = f\"{class_names[idx]} : {last_preds[idx]*100:.1f}%\"\n",
    "            y_pos = 40 + i * 35\n",
    "            color = (0, 255, 0) if i == 0 else (255, 255, 255)\n",
    "            cv2.putText(out, text, (20, y_pos),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2, cv2.LINE_AA)\n",
    "\n",
    "    # --- Affichage caméra ---\n",
    "    cv2.imshow(\"Reconnaissance de gestes\", out)\n",
    "    if cv2.waitKey(5) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfmetal310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
