{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fdf8766",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow import keras\n",
    "from keras import layers, preprocessing, callbacks, losses, utils, models\n",
    "import sys\n",
    "import PyQt5\n",
    "from PyQt5.QtWidgets import QApplication, QLabel, QWidget, QVBoxLayout, QHBoxLayout, QGridLayout,QFrame\n",
    "from PyQt5.QtGui import QPixmap, QPainter, QPen, QColor\n",
    "from PyQt5.QtCore import Qt\n",
    "import time\n",
    "import glob\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79400eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 1) Parsing des fichiers\n",
    "# =========================\n",
    "\n",
    "def load_sequence_txt(path):\n",
    "    \"\"\"\n",
    "    Lit un .txt où chaque ligne est une frame : \"v1;v2;...;vN;\"\n",
    "    -> retourne un numpy array (T, D) où T = nb de frames, D = nb de features par frame\n",
    "    \"\"\"\n",
    "    feats = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            parts = [p for p in line.strip().split(\";\") if p.strip() != \"\"]\n",
    "            if not parts:\n",
    "                continue\n",
    "            vec = list(map(float, parts))\n",
    "            feats.append(vec)\n",
    "    X = np.array(feats, dtype=np.float32)  # (T, D)\n",
    "    return X\n",
    "\n",
    "def parse_annotations(path):\n",
    "    \"\"\"\n",
    "    Fichier d’annotations, format (extraits) :\n",
    "      1;DENY;670;751;CIRCLE;1610;1655; ...\n",
    "    => Pour chaque ID, on récupère une liste [(label, start, end), ...]\n",
    "    \"\"\"\n",
    "    ann = defaultdict(list)\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for raw in f:\n",
    "            raw = raw.strip()\n",
    "            if not raw:\n",
    "                continue\n",
    "            parts = [p for p in raw.split(\";\") if p != \"\"]\n",
    "            # parts = [id, L1, s1, e1, L2, s2, e2, ...]\n",
    "            seq_id = parts[0]\n",
    "            triples = parts[1:]\n",
    "            # lire par paquets de 3 (label, start, end)\n",
    "            for i in range(0, len(triples), 3):\n",
    "                label = triples[i]\n",
    "                start = int(triples[i+1])\n",
    "                end   = int(triples[i+2])\n",
    "                ann[seq_id].append((label, start, end))\n",
    "    return ann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561e6f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 2) Prétraitements\n",
    "# =========================\n",
    "\n",
    "def normalize_framewise(X):\n",
    "    \"\"\"\n",
    "    Normalisation simple frame-par-frame :\n",
    "    - centrage par la moyenne\n",
    "    - mise à l’échelle par l’écart-type\n",
    "    Remarque : si tes features sont des coordonnées articulaires (x,y,z) concaténées,\n",
    "    tu peux remplacer par un centrage/scale géométrique (soustraire le 'poignet', etc.).\n",
    "    \"\"\"\n",
    "    Xn = X.copy()\n",
    "    mu = Xn.mean(axis=1, keepdims=True)      # (T,1)\n",
    "    sigma = Xn.std(axis=1, keepdims=True) + 1e-8\n",
    "    Xn = (Xn - mu) / sigma\n",
    "    return Xn\n",
    "\n",
    "def slice_segment(X, start, end):\n",
    "    \"\"\"\n",
    "    Découpe X (T,D) sur l’intervalle [start, end] inclusif (ou semi-ouvert)\n",
    "    On borne pour éviter les dépassements.\n",
    "    \"\"\"\n",
    "    T = len(X)\n",
    "    s = max(0, start)\n",
    "    e = min(T, end)\n",
    "    if e <= s:\n",
    "        return None\n",
    "    return X[s:e]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4970997e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 3) Construction dataset\n",
    "# =========================\n",
    "\n",
    "def build_dataset_from_folder(seq_folder, ann_path, max_len=200):\n",
    "    \"\"\"\n",
    "    - lit toutes les séquences *.txt du dossier\n",
    "    - récupère les segments annotés pour chaque ID\n",
    "    - normalise, pad/tronque à max_len\n",
    "    Retourne X (N, max_len, D), y (N,), class_names\n",
    "    \"\"\"\n",
    "    annotations = parse_annotations(ann_path)\n",
    "    sequences = sorted(glob.glob(os.path.join(seq_folder, \"*.txt\")))\n",
    "    X_list, y_list = [], []\n",
    "    label_to_id = {}\n",
    "    next_id = 0\n",
    "\n",
    "    for path in sequences:\n",
    "        # ID = nom de fichier sans extension\n",
    "        seq_id = os.path.splitext(os.path.basename(path))[0]\n",
    "        if seq_id not in annotations:\n",
    "            continue\n",
    "        X = load_sequence_txt(path)          # (T, D)\n",
    "        X = normalize_framewise(X)\n",
    "\n",
    "        for (label, start, end) in annotations[seq_id]:\n",
    "            seg = slice_segment(X, start, end)\n",
    "            if seg is None: \n",
    "                continue\n",
    "            # padding/tronquage à max_len\n",
    "            if len(seg) >= max_len:\n",
    "                seg = seg[:max_len]\n",
    "            else:\n",
    "                pad = np.zeros((max_len - len(seg), seg.shape[1]), dtype=seg.dtype)\n",
    "                seg = np.vstack([seg, pad])\n",
    "            # map label -> id\n",
    "            if label not in label_to_id:\n",
    "                label_to_id[label] = next_id\n",
    "                next_id += 1\n",
    "            y = label_to_id[label]\n",
    "            X_list.append(seg)\n",
    "            y_list.append(y)\n",
    "\n",
    "    if not X_list:\n",
    "        raise RuntimeError(\"Aucun segment construit (vérifie les chemins et les IDs).\")\n",
    "\n",
    "    X = np.stack(X_list, axis=0)          # (N, max_len, D)\n",
    "    y = np.array(y_list, dtype=np.int64)  # (N,)\n",
    "    # classes triées par id\n",
    "    id_to_label = {v:k for k,v in label_to_id.items()}\n",
    "    class_names = [id_to_label[i] for i in range(len(id_to_label))]\n",
    "    return X, y, class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b6e694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 4) Modèle (BiLSTM)\n",
    "# =========================\n",
    "\n",
    "def make_model(T, D, C):\n",
    "    \"\"\"\n",
    "    T = longueur max (max_len)\n",
    "    D = nb features par frame\n",
    "    C = nb de classes\n",
    "    \"\"\"\n",
    "    inp = layers.Input(shape=(T, D))\n",
    "    x = layers.Masking(mask_value=0.0)(inp)\n",
    "    x = layers.Bidirectional(layers.LSTM(128, return_sequences=True))(x)\n",
    "    x = layers.Bidirectional(layers.LSTM(64))(x)\n",
    "    x = layers.Dense(64, activation='relu')(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    out = layers.Dense(C, activation='softmax', dtype='float32')(x)\n",
    "    model = models.Model(inp, out)\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29aea3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 5) Entraînement + test\n",
    "# =========================\n",
    "\n",
    "# Dossier où se trouvent 1.txt, 2.txt, ...\n",
    "seq_folder = \"/Users/valentindaveau/Documents/UE_Vision/training_set/sequences/\"  # adapte si besoin\n",
    "ann_path   = \"annotations_revised_training.txt\"\n",
    "\n",
    "# Construire le dataset\n",
    "X, y, class_names = build_dataset_from_folder(seq_folder, ann_path, max_len=200)\n",
    "print(\"Dataset:\", X.shape, y.shape, class_names)\n",
    "\n",
    "# Split simple (train/val)\n",
    "idx = np.arange(len(X))\n",
    "np.random.shuffle(idx)\n",
    "split = int(0.8 * len(X))\n",
    "tr, va = idx[:split], idx[split:]\n",
    "Xtr, ytr = X[tr], y[tr]\n",
    "Xva, yva = X[va], y[va]\n",
    "\n",
    "T, D = X.shape[1], X.shape[2]\n",
    "C    = len(class_names)\n",
    "\n",
    "model = make_model(T, D, C)\n",
    "history = model.fit(Xtr, ytr, validation_data=(Xva, yva), epochs=20, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26c617d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 6) Enlève index 3 (pouce) de MediaPipe\n",
    "# =========================\n",
    "\n",
    "\n",
    "KEEP_IDX_20 = [0, 1, 2, 4,  5,6,7,8,  9,10,11,12,  13,14,15,16,  17,18,19,20]\n",
    "\n",
    "def mp21_to_dataset20(mp21_xyz: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    mp21_xyz: array (21, 3) en ordre MediaPipe (channels_last)\n",
    "    retourne: array (20, 3) dans l’ordre de ton dataset\n",
    "    \"\"\"\n",
    "    assert mp21_xyz.shape == (21, 3), f\"attendu (21,3), reçu {mp21_xyz.shape}\"\n",
    "    return mp21_xyz[KEEP_IDX_20, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1287465a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 7) Normalisation données Mediapipe\n",
    "# =========================\n",
    "\n",
    "\n",
    "def normalize_landmarks(X20: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    X20: (20,3) ordre dataset.\n",
    "    - centre sur le poignet (point 0)\n",
    "    - mise à l’échelle par une distance de référence stable (ex: WRIST→MIDDLE_MCP)\n",
    "    \"\"\"\n",
    "    wrist = X20[0]\n",
    "    Xc = X20 - wrist\n",
    "    # distance de ref: WRIST (0) -> MIDDLE MCP (index MediaPipe 9, devenu X20[8] après mapping)\n",
    "    scale = np.linalg.norm(Xc[8]) + 1e-8\n",
    "    return Xc / scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45407b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 8) Inférence (sortie)\n",
    "# =========================\n",
    "\n",
    "def predict_sequence(model, path_txt, max_len=200):\n",
    "    X = load_sequence_txt(path_txt)\n",
    "    X = normalize_framewise(X)\n",
    "    # simple agrégation : on prend toute la séquence tronquée/paddée\n",
    "    if len(X) >= max_len:\n",
    "        Xp = X[:max_len]\n",
    "    else:\n",
    "        pad = np.zeros((max_len - len(X), X.shape[1]), dtype=X.dtype)\n",
    "        Xp = np.vstack([X, pad])\n",
    "    Xp = np.expand_dims(Xp, axis=0)  # (1, T, D)\n",
    "    prob = model.predict(Xp, verbose=0)[0]  # (C,)\n",
    "    k = int(np.argmax(prob))\n",
    "    return k, float(prob[k])\n",
    "\n",
    "k, p = predict_sequence(model, \"1.txt\", max_len=200)\n",
    "print(f\"Prédiction pour 1.txt : {class_names[k]} (p={p:.2f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfmetal310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
