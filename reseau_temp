import os
import pathlib
import numpy as np
import cv2
from tensorflow import keras
import tensorflow as tf
from keras import layers, preprocessing, callbacks, losses, utils, models
import sys
import PyQt5
from PyQt5.QtWidgets import QApplication, QLabel, QWidget, QVBoxLayout, QHBoxLayout, QGridLayout,QFrame
from PyQt5.QtGui import QPixmap, QPainter, QPen, QColor
from PyQt5.QtCore import Qt
import time
import glob
from collections import defaultdict


# D√©tection automatique du GPU Apple (Metal)
devices = tf.config.list_physical_devices()
gpu_devices = [d for d in devices if d.device_type == "GPU"]
if gpu_devices:
    print("‚úÖ GPU Apple d√©tect√© :", gpu_devices)
else:
    print("‚ö†Ô∏è Aucun GPU Apple d√©tect√©, utilisation du CPU.")

# =========================
# 1) Parsing des fichiers
# =========================

def load_sequence_txt(path):
    """
    Lit un .txt o√π chaque ligne est une frame : "v1;v2;...;vN;" ou "v1 v2 ... vN"
    -> retourne un numpy array (T, D) o√π T = nb de frames, D = nb de features par frame
    """
    feats = []
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            # D√©tection automatique du s√©parateur
            if ";" in line:
                parts = [p for p in line.split(";") if p.strip() != ""]
            else:
                parts = [p for p in line.split() if p.strip() != ""]
            try:
                vec = list(map(float, parts))
            except ValueError:
                print(f"‚ö†Ô∏è Ligne ignor√©e dans {path}: {line[:50]}...")
                continue
            feats.append(vec)
    X = np.array(feats, dtype=np.float32)
    return X

# =========================
# 2) Pr√©traitements
# =========================

def normalize_framewise(X):
    """
    Normalisation simple frame-par-frame :
    - centrage par la moyenne
    - mise √† l‚Äô√©chelle par l‚Äô√©cart-type
    Remarque : si tes features sont des coordonn√©es articulaires (x,y,z) concat√©n√©es,
    tu peux remplacer par un centrage/scale g√©om√©trique (soustraire le 'poignet', etc.).
    """
    Xn = X.copy()
    mu = Xn.mean(axis=0, keepdims=True)      # (1, D)
    sigma = Xn.std(axis=0, keepdims=True) + 1e-8
    Xn = (Xn - mu) / sigma
    return Xn

def load_dataset_from_folder(seq_folder, max_len=200):
    """
    Charge toutes les s√©quences *.txt du dossier.
    Chaque fichier correspond √† un seul geste, dont le label est le nom du fichier (sans extension).
    Normalise chaque s√©quence.
    Pad/tronque √† max_len.
    Retourne X (N, max_len, D), y (N,), class_names
    """
    sequences = sorted(glob.glob(os.path.join(seq_folder, "*.txt")))
    X_list, y_list = [], []
    label_to_id = {}
    next_id = 0

    for path in sequences:
        label = os.path.splitext(os.path.basename(path))[0]
        X = load_sequence_txt(path)
        X = normalize_framewise(X)

        # padding/tronquage √† max_len
        if len(X) >= max_len:
            seg = X[:max_len]
        else:
            pad = np.zeros((max_len - len(X), X.shape[1]), dtype=X.dtype)
            seg = np.vstack([X, pad])

        if label not in label_to_id:
            label_to_id[label] = next_id
            next_id += 1
        y = label_to_id[label]
        X_list.append(seg)
        y_list.append(y)

    if not X_list:
        raise RuntimeError("Aucun segment construit (v√©rifie les chemins et les fichiers).")

    X = np.stack(X_list, axis=0)          # (N, max_len, D)
    y = np.array(y_list, dtype=np.int64)  # (N,)
    id_to_label = {v:k for k,v in label_to_id.items()}
    class_names = [id_to_label[i] for i in range(len(id_to_label))]
    return X, y, class_names

# =========================
# 4) Mod√®le (BiLSTM)
# =========================

def make_model(T, D, C):
    """
    T = longueur max (max_len)
    D = nb features par frame
    C = nb de classes
    """
    inp = layers.Input(shape=(T, D))
    x = layers.Masking(mask_value=0.0)(inp)
    x = layers.Bidirectional(layers.LSTM(128, return_sequences=True))(x)
    x = layers.Bidirectional(layers.LSTM(64))(x)
    x = layers.Dense(64, activation='relu')(x)
    x = layers.Dropout(0.3)(x)
    out = layers.Dense(C, activation='softmax', dtype='float32')(x)
    model = models.Model(inp, out)
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    return model

# =========================
# 5) Entra√Ænement + test
# =========================

# Dossier o√π se trouvent les fichiers *.txt
seq_folder = "/Users/valentindaveau/Documents/UE_Vision/Coords" 

# Construire le dataset
X, y, class_names = load_dataset_from_folder(seq_folder, max_len=200)
print("Dataset:", X.shape, y.shape, class_names)

# Split simple (train/val)
idx = np.arange(len(X))
np.random.shuffle(idx)
split = int(0.8 * len(X))
tr, va = idx[:split], idx[split:]
Xtr, ytr = X[tr], y[tr]
Xva, yva = X[va], y[va]

T, D = X.shape[1], X.shape[2]
C    = len(class_names)

model_path = "modele_gestes.keras"

if os.path.exists(model_path):
    print("‚úÖ Mod√®le trouv√©, chargement du mod√®le sauvegard√©...")
    model = keras.models.load_model(model_path)
    class_names = np.load("class_names.npy", allow_pickle=True).tolist()
    print("‚úÖ Noms de classes charg√©s :", class_names)
else:
    print("‚öôÔ∏è  Aucun mod√®le trouv√©, entra√Ænement en cours...")
    model = make_model(T, D, C)
    history = model.fit(Xtr, ytr, validation_data=(Xva, yva), epochs=20, batch_size=32)
    model.save(model_path)
    print(f"üíæ Mod√®le sauvegard√© sous : {model_path}")
    np.save("class_names.npy", class_names)
    print("üíæ Noms de classes sauvegard√©s dans class_names.npy")



def normalize_landmarks(X20: np.ndarray) -> np.ndarray:
    """
    X20: (20,3) ordre dataset.
    - centre sur le poignet (point 0)
    - mise √† l‚Äô√©chelle par une distance de r√©f√©rence stable (ex: WRIST‚ÜíMIDDLE_MCP)
    """
    wrist = X20[0]
    Xc = X20 - wrist
    # distance de ref: WRIST (0) -> MIDDLE MCP (index MediaPipe 9, devenu X20[8] apr√®s mapping)
    scale = np.linalg.norm(Xc[8]) + 1e-8
    return Xc / scale

# =========================
# 8) Inf√©rence (sortie)
# =========================

def predict_sequence(model, path_txt, max_len=200):
    X = load_sequence_txt(path_txt)
    X = normalize_framewise(X)
    # simple agr√©gation : on prend toute la s√©quence tronqu√©e/padd√©e
    if len(X) >= max_len:
        Xp = X[:max_len]
    else:
        pad = np.zeros((max_len - len(X), X.shape[1]), dtype=X.dtype)
        Xp = np.vstack([X, pad])
    Xp = np.expand_dims(Xp, axis=0)  # (1, T, D)
    prob = model.predict(Xp, verbose=0)[0]  # (C,)
    k = int(np.argmax(prob))
    return k, float(prob[k])

k, p = predict_sequence(model, "1.txt", max_len=200)
print(f"Pr√©diction pour 1.txt : {class_names[k]} (p={p:.2f})")